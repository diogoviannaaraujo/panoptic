{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video face tracking and recognition\n",
    "\n",
    "The idea of this project is to build a example flow for understanding of real time video.\n",
    "\n",
    "The process consists of some steps:\n",
    "Person Detection -> Face detection -> Face recognition -> Face store\n",
    "  |-> Video Understanding -> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "We need a virtual environment to isolate from system installed pip packages, we are using python3 venv package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual Environment Created\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "!apt update > /dev/null\n",
    "!apt install -y python3.10-venv cmake\n",
    "!python -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "clear_output(wait=True)\n",
    "print(\"Virtual Environment Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inialize face detection and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "print(\"asdf\")\n",
    "base_model = YOLO(\"yolov11n-face.pt\")\n",
    "base_model.export(format=\"engine\")\n",
    "model = YOLO(\"yolov11n-face.engine\")\n",
    "stream = model.track(\"./bar_cam_1.mp4\",\n",
    "                      conf=0.75,\n",
    "                      stream=True,\n",
    "                      #verbose=False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detections = {}\n",
    "\n",
    "for frame_i, detection in enumerate(stream):\n",
    "    boxes = detection.boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        detectionid = int(box.id.item()) if hasattr(box, 'id') and box.id is not None else None\n",
    "        confidence = float(box.conf.item()) if hasattr(box, 'conf') else None\n",
    "\n",
    "        if detectionid is None:\n",
    "            continue\n",
    "        \n",
    "        if detectionid not in detections:\n",
    "            detections[detectionid] = {}\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy.cpu().numpy()[0])\n",
    "        face = detection.orig_img[y1:y2, x1:x2]\n",
    "\n",
    "        detections[detectionid][frame_i] = {\n",
    "            'face': face,\n",
    "            'confidence': confidence\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display faces\n",
    "Display the detected face tracking groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "filtered_detections = {}\n",
    "\n",
    "def reduce_list(items, factor=2, limit=200):\n",
    "    n = len(items)\n",
    "    \n",
    "    # Determine how many items to keep\n",
    "    keep_count = max(10, min(int(np.sqrt(n) * factor), limit))\n",
    "\n",
    "    # If we don't need to reduce, return the original list\n",
    "    if keep_count >= n:\n",
    "        return items\n",
    "    \n",
    "    # Select `keep_count` items spaced evenly across the list\n",
    "    indices = np.linspace(0, n - 1, keep_count, dtype=int)\n",
    "    return [items[i] for i in indices]\n",
    "\n",
    "# Filter frames based on confidence and reduce list size\n",
    "for detectionid, frames in detections.items():\n",
    "    # Filter out low-confidence frames\n",
    "    filtered_frames = {frame_i: face_data for frame_i, face_data in frames.items() if face_data['confidence'] >= 0.8}\n",
    "    \n",
    "    # Only keep detections with at least 10 frames after filtering\n",
    "    if len(filtered_frames) >= 6:\n",
    "        # Sort by frame number and reduce list size\n",
    "        reduced_frames = reduce_list(sorted(filtered_frames.items()))\n",
    "\n",
    "        # Convert back to dictionary format\n",
    "        filtered_detections[detectionid] = dict(reduced_frames)\n",
    "\n",
    "# Display detections\n",
    "for detectionid, frames in filtered_detections.items():\n",
    "    print(f\"Detection: {detectionid}\")\n",
    "    num_frames = len(frames)\n",
    "    cols = min(16, num_frames)  # Set max 16 columns per row\n",
    "    rows = math.ceil(num_frames / cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "\n",
    "    # Flatten axes for easy indexing\n",
    "    axes = axes.flatten() if num_frames > 1 else [axes]\n",
    "\n",
    "    for i, (frame_i, face_data) in enumerate(frames.items()):\n",
    "        #confidence = \"{:.2f}\".format(face_data['confidence'])\n",
    "        axes[i].imshow(cv2.cvtColor(face_data['face'], cv2.COLOR_RGB2BGR))  # Convert BGR to RGB if using OpenCV\n",
    "        axes[i].axis(\"off\")\n",
    "        #axes[i].set_title(confidence)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(num_frames, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insightface.model_zoo import model_zoo\n",
    "\n",
    "rec_model_path = '/kaggle/input/insightface-buffalo_l/onnx/default/1/w600k_r50.onnx'\n",
    "rec_model = model_zoo.get_model(rec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./facedb\")\n",
    "facedb = client.get_or_create_collection(\n",
    "    name='facedb',\n",
    "    metadata={\n",
    "        \"hnsw:space\": 'cosine',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "for detectionid, frames in filtered_detections.items():\n",
    "    query_results = []\n",
    "    print(f\"Detection {detectionid}\")\n",
    "    for i, (frame_i, face_data) in enumerate(frames.items()):\n",
    "        face = face_data['face']\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "        known_face_location = [(0, face.shape[1], face.shape[0], 0)]\n",
    "        embed = face_recognition.face_encodings(face, known_face_locations=known_face_location)[0]\n",
    "\n",
    "        qresult = facedb.query(\n",
    "            query_embeddings=[embed],\n",
    "            n_results=20\n",
    "        )\n",
    "        print(qresult)\n",
    "        print(\"\\n\")\n",
    "        query_results.append(qresult)\n",
    "        \n",
    "        # facedb.add(\n",
    "        #     ids=[str(detectionid) + \":\" +  str(frame_i)],\n",
    "        #     embeddings=[embed],  # Replace with your embeddings\n",
    "        #     metadatas=[{'detection': detectionid}]\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7026744,
     "sourceId": 11245920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
